{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "0. Getting Started.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiminPark/bandit-reco/blob/master/notebooks/0.%20Getting%20Started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWmmhpKtDaDh",
        "outputId": "77e76c9d-99d7-4477-ea5a-a45e6c64a624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If recogym is not yet installed in your environment, run:\n",
        "!pip install recogym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting recogym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/d1/18bc332726ca043508ad13266bc3709265cffdc6dba016bd592f7a48ba67/recogym-0.1.3.0-py3-none-any.whl (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 20kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 30kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 40kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
            "\u001b[?25hCollecting datetime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/22/a5297f3a1f92468cc737f8ce7ba6e5f245fcfafeae810ba37bd1039ea01c/DateTime-4.3-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from recogym) (1.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from recogym) (3.2.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from recogym) (0.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from recogym) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from recogym) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from recogym) (1.7.0+cu101)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from recogym) (1.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from recogym) (0.48.0)\n",
            "Requirement already satisfied: simplegeneric in /usr/local/lib/python3.6/dist-packages (from recogym) (0.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from recogym) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from recogym) (0.10.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from recogym) (2.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from recogym) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from datetime->recogym) (2018.9)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->recogym) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->recogym) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->recogym) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->recogym) (0.10.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->recogym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->recogym) (1.5.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->recogym) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->recogym) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->recogym) (0.16.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->recogym) (5.0.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->recogym) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->recogym) (7.5.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->recogym) (4.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->recogym) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->recogym) (5.3.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->recogym) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->recogym) (50.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->recogym) (1.15.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (1.33.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (2.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->recogym) (0.35.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->recogym) (0.17.0)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->recogym) (5.3.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->recogym) (2.6.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->recogym) (1.9.0)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->recogym) (4.3.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->recogym) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->recogym) (20.0.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->recogym) (4.7.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->recogym) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->recogym) (1.0.18)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->recogym) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->recogym) (5.0.8)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->recogym) (5.1.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->recogym) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->recogym) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->recogym) (3.2.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->recogym) (2.11.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->recogym) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->recogym) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->recogym) (0.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->recogym) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->recogym) (1.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->recogym) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->recogym) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->recogym) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->recogym) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->recogym) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->recogym) (2.23.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter->recogym) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->recogym) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->recogym) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->recogym) (0.2.5)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->recogym) (2.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->recogym) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->recogym) (20.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->recogym) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->recogym) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->recogym) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->recogym) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->recogym) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->recogym) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->recogym) (0.2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->recogym) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->recogym) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->recogym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->recogym) (2020.11.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->recogym) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->recogym) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->recogym) (0.4.8)\n",
            "Installing collected packages: zope.interface, datetime, recogym\n",
            "Successfully installed datetime-4.3 recogym-0.1.3.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMt1yeoHDaDh"
      },
      "source": [
        "## What is RecoGym?\n",
        "\n",
        "RecoGym is a Python [OpenAI Gym](https://gym.openai.com/) environment for testing recommendation algorithms.  It allows for the testing of both offline and reinforcement-learning based agents.  It provides a way to test algorithms in a toy environment quickly.\n",
        "\n",
        "In this notebook, we will code a simple recommendation agent that suggests an item in proportion to how many times it has been viewed.  We hope to inspire you to create your agents and test them against our baseline models.\n",
        "\n",
        "To make the most out of RecoGym, we suggest you have some experience coding in Python, some background knowledge in recommender systems, and familiarity with the reinforcement learning setup.  Also, be sure to check out the python-based requirements in the README if something below errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ0WSc_xDaDh"
      },
      "source": [
        "## Reinforcement Learning Setup\n",
        "\n",
        "RecoGym follows the usual reinforcement learning setup.  It means there are interactions between the environment (the user's behavior) and the agent (our recommendation algorithm).  The agent receives a reward if the user clicks on the recommendation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147cusrsDaDh"
      },
      "source": [
        "<img src=\"https://github.com/ZiminPark/bandit-reco/blob/master/notebooks/images/rl-setup.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_H1djTJDaDh"
      },
      "source": [
        "## Organic and Bandit\n",
        "\n",
        "Even though our focus is biased towards online advertising, we tried to make RecoGym universal to all types of recommendation.  Hence, we introduce the domain-agnostic terms _Organic_ and _Bandit_ sessions.  An _Organic_ session is an observation of items the user interacts with.  For example, it could be views of products on an e-commerce website, listens to songs while streaming music, or readings of articles on an online newspaper.  A _Bandit_ session is one where we have an opportunity to recommend the user an item and observe their behavior.  We receive a reward if they click.\n",
        "\n",
        "<img src=\"https://github.com/ZiminPark/bandit-reco/blob/master/notebooks/images/organic-bandit.png?raw=1\" alt=\"Drawing\" style=\"width: 450px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F2YNvlpDaDh"
      },
      "source": [
        "## Offline and Online Learning\n",
        "\n",
        "This project was born out of a desire to improve Criteo's recommendation system by exploring reinforcement learning algorithms. We quickly realized that we couldn't just blindly apply RL algorithms in a production system out of the box. The learning period would be too costly. Instead, we need to leverage the vast amounts of offline training examples we already to make the algorithm perform as good as the current system before releasing it into the online production environment.\n",
        "\n",
        "Thus, RecoGym follows a similar flow. An agent is first given access to many offline training examples produced from a fixed policy. Then, they have access to the online system where they choose the actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x0_yDUyDaDh"
      },
      "source": [
        "<img src=\"https://github.com/ZiminPark/bandit-reco/blob/master/notebooks/images/two-steps.png?raw=1\" alt=\"Drawing\" style=\"width: 450px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnqqyW6PDaDi"
      },
      "source": [
        "## Let's see some code - Interacting with the environment \n",
        "\n",
        "\n",
        "The code snippet below shows how to initialize the environment and step through in an 'offline' manner (Here offline means that the environment is generating some recommendations for us).  We print out the results from the environment at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Blq14poDaDi"
      },
      "source": [
        "### World creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GqjZJTNDaDi"
      },
      "source": [
        "import gym, recogym\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IQnDfI7DaDi"
      },
      "source": [
        "# env_1_args is a dictionary of default parameters that defines the simulated world (such as user behavior, number of products, etc.).\n",
        "from recogym import env_1_args, Configuration\n",
        "\n",
        "# You can overwrite environment arguments here:\n",
        "env_1_args['random_seed'] = 42\n",
        "\n",
        "# Initialize the gym for the first time by calling .make() and .init_gym()\n",
        "env = gym.make('reco-gym-v1')\n",
        "env.init_gym(env_1_args)\n",
        "\n",
        "# .reset() env before each episode (one episode per user).\n",
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRA_vktkDaDi"
      },
      "source": [
        "### Act on the environment\n",
        "We will now choose the product to recommend, and _hope_ for a click from the user.\n",
        "For our first agent we will hardcode the actions taken."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL420I62DaDi",
        "outputId": "8b00f510-060f-45de-b491-ed456f69eb5e"
      },
      "source": [
        "# Create a list of hard coded actions.\n",
        "actions = [None] + [1, 2, 3, 4, 5]\n",
        "\n",
        "# Reset the environment and set Done to False.\n",
        "env.reset()\n",
        "done = False\n",
        "\n",
        "# Counting how many steps.\n",
        "i = 0\n",
        "\n",
        "while not done and i < len(actions):\n",
        "    action = actions[i]\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    print(f\"Step: {i} - Action: {action} - Observation: {observation.sessions()} - Reward: {reward}\")\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 - Action: None - Observation: [{'t': 0, 'u': 0, 'z': 'pageview', 'v': 1}, {'t': 1, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 2, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 3, 'u': 0, 'z': 'pageview', 'v': 1}, {'t': 4, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 5, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 6, 'u': 0, 'z': 'pageview', 'v': 1}] - Reward: None\n",
            "Step: 1 - Action: 1 - Observation: [] - Reward: 0\n",
            "Step: 2 - Action: 2 - Observation: [] - Reward: 0\n",
            "Step: 3 - Action: 3 - Observation: [] - Reward: 0\n",
            "Step: 4 - Action: 4 - Observation: [] - Reward: 0\n",
            "Step: 5 - Action: 5 - Observation: [] - Reward: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5P_MkvoDaDi"
      },
      "source": [
        "Okay, there's quite a bit going on here:  \n",
        "- `Action`\n",
        "   * `t` is the timestep (always incremented), it won't be useful today\n",
        "   * `u` is the user id, as we have one user, for now, it's always 0\n",
        "   * `a` is a number between `0` and `num_products - 1` that references the index of the product recommended.\n",
        "   * `ps` is the propensity score or the probability that the agent assigned to this action\n",
        "   * `ps-a` are the probabilities assigned to all actions by the agent (we can see that it's uniform for now: the agent randomly selects the recommended product)\n",
        "- `observation` will either be `None` or a session of Organic data, showing the index of products the user views.\n",
        "   * `t`, `u` have the same meaning as above\n",
        "   * `z` in the type of event (always pageview for now)\n",
        "   * `v` is the index of the viewed product\n",
        "- `reward` is `0` if the user does not click on the recommended product and `1` if they do.  Notice that when a user clicks on a product (Wherever the reward is `1`), they start a new Organic session.\n",
        "- `done` is a True/False flag indicating if the episode (aka user's timeline) is over.  \n",
        "\n",
        "Also, notice that the first `action` is `None`.  In our implementation, the agent observes Organic behavior before recommending anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCGTkfPEDaDi"
      },
      "source": [
        "## Creating our first agent\n",
        "\n",
        "Now that we have seen how the offline and online versions of the environment work, it is time to code our first recommendation agent!  Technically, an agent can be anything that produces actions for the environment to use.  However, we will show you the object-oriented way we like to create agents.\n",
        "\n",
        "Below is the code for a very simple agent - the _best of_ agent. The _best of_ agent records merely how many times each product has been seen organically, then when required to make a recommendation, the agent chooses a product randomly in proportion with a number of times it has been viewed overall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLydgBreDaDi"
      },
      "source": [
        "import numpy as np\n",
        "from recogym.agents import Agent\n",
        "\n",
        "# Define an Agent class.\n",
        "class BestOfAgent(Agent):\n",
        "    def __init__(self, config):\n",
        "        # Set number of products as an attribute of the Agent.\n",
        "        Agent.__init__(self, config)\n",
        "\n",
        "        # Track number of times each item viewed in the organic sessions.\n",
        "        self.organic_views = np.zeros(self.config.num_products)\n",
        "\n",
        "    def train(self, observation, action, reward, done):\n",
        "        \"\"\"Train method learns from a tuple of data.\n",
        "        this method can be called for offline or online learning\n",
        "        \"\"\"\n",
        "        # Adding organic session to organic view counts.\n",
        "        if observation:\n",
        "            for session in observation.sessions():\n",
        "                viewed_item_index = session['v']\n",
        "                self.organic_views[viewed_item_index] += 1\n",
        "\n",
        "    def act(self, observation, reward, done):\n",
        "        \"\"\"Act method returns an action based on current observation and past\n",
        "        history\n",
        "        \"\"\"\n",
        "        # Choosing action randomly in proportion with number of views.\n",
        "        probabilities = self.organic_views / np.sum(self.organic_views)\n",
        "        action = np.random.choice(self.config.num_products, p=probabilities)\n",
        "        \n",
        "        return {\n",
        "            **super().act(observation, reward, done),\n",
        "            **{\n",
        "                'a': action,\n",
        "                'ps': probabilities[action],\n",
        "                'ps-a': probabilities,\n",
        "            }\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exa_kiTJDaDj"
      },
      "source": [
        "The `BestOfAgent` class above demonstrates our preferred way to create agents for RecoGym. Notice how we have both a `train` and `act` method present. The `train` method is designed to take in training data from the environments `step_offline` method and thus has nothing to return, while the `act` method must return an action to pass back into the environment. \n",
        "\n",
        "The code below highlights how one would use this agent for first offline training and then using the learned knowledge to make recommendations online."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLgbWCLHDaDj",
        "outputId": "6f3e2076-28e0-4910-f128-72bed8812109"
      },
      "source": [
        "# Instantiate instance of PopularityAgent class.\n",
        "num_products = 10\n",
        "agent = BestOfAgent(Configuration({\n",
        "    **env_1_args,\n",
        "    'num_products': num_products,\n",
        "}))\n",
        "\n",
        "# Resets random seed back to 42, or whatever we set it to in env_0_args.\n",
        "env.reset_random_seed()\n",
        "\n",
        "# Train on 1000 users offline.\n",
        "num_offline_users = 1000\n",
        "\n",
        "for _ in range(num_offline_users):\n",
        "\n",
        "    # Reset env and set done to False.\n",
        "    env.reset()\n",
        "    done = False\n",
        "\n",
        "    observation, reward, done = None, 0, False\n",
        "    while not done:\n",
        "        old_observation = observation\n",
        "        action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
        "        agent.train(old_observation, action, reward, done)\n",
        "\n",
        "# Train on 100 users online and track click through rate.\n",
        "num_online_users = 100\n",
        "num_clicks, num_events = 0, 0\n",
        "\n",
        "for _ in range(num_online_users):\n",
        "\n",
        "    # Reset env and set done to False.\n",
        "    env.reset()\n",
        "    observation, _, done, _ = env.step(None)\n",
        "    reward = None\n",
        "    done = None\n",
        "    while not done:\n",
        "        action = agent.act(observation, reward, done)\n",
        "        observation, reward, done, info = env.step(action['a'])\n",
        "\n",
        "        # Used for calculating click through rate.\n",
        "        num_clicks += 1 if reward == 1 else 0\n",
        "        num_events += 1\n",
        "\n",
        "ctr = num_clicks / num_events\n",
        "\n",
        "print(f\"Click Through Rate: {ctr:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Click Through Rate: 0.0142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTXKnE_TDaDj"
      },
      "source": [
        "## Testing our first agent\n",
        "\n",
        "Now we have created our popularity based agent, and we should test it against an even simpler baseline - one that performs no learning and recommends products uniformly at random. To do this, we will first load a more complex version of the toy data environment called `reco-gym-v1`.\n",
        "\n",
        "Next, we will load another agent for our agent to compete against each other. Here you can see we make use of the `RandomAgent` and create an instance of it in addition to our `BestOfAgent`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJJZC8UHDaDj"
      },
      "source": [
        "import gym, recogym\n",
        "from recogym import env_1_args\n",
        "from recogym.agents import RandomAgent, random_args\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "env_1_args['random_seed'] = 42\n",
        "\n",
        "env_1 = gym.make('reco-gym-v1')\n",
        "env_1.init_gym(env_1_args)\n",
        "\n",
        "# Create the two agents.\n",
        "num_products = env_1_args['num_products']\n",
        "\n",
        "best_of_agent = BestOfAgent(Configuration(env_1_args))\n",
        "random_agent = RandomAgent(Configuration({\n",
        "    **env_1_args,\n",
        "    **random_args,\n",
        "}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyMKCQkUDaDj"
      },
      "source": [
        "Now we have instances of our two agents. We can use the `test_agent` method from RecoGym and compare their performance.\n",
        "\n",
        "To use `test_agent`, one must provide a copy of the current env, a copy of the agent class, the number of training users, and the number of testing users. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbITkmXXDaDj",
        "outputId": "133921a6-bf16-4681-b354-5bea06f9e77d"
      },
      "source": [
        "# Confidence interval of the CTR median and 0.025 0.975 quantile.\n",
        "random_agent_results = recogym.test_agent(\n",
        "    deepcopy(env_1),\n",
        "    deepcopy(random_agent),\n",
        "    num_offline_users=1000,\n",
        "    num_online_users=1000\n",
        ")\n",
        "median_random_agent, lower_bound_random_agent, upper_bound_random_agent = random_agent_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start: Agent Training #0\n",
            "Start: Agent Testing #0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6OuEdjXDaDj"
      },
      "source": [
        "# Confidence interval of the CTR median and 0.025 0.975 quantile.\n",
        "bestof_agent_results = recogym.test_agent(\n",
        "    deepcopy(env),\n",
        "    deepcopy(best_of_agent),\n",
        "    num_offline_users=1000,\n",
        "    num_online_users=1000\n",
        ")\n",
        "median_bestof_agent, lower_bound_bestof_agent, upper_bound_bestof_agent = bestof_agent_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlnDnVTSDaDj"
      },
      "source": [
        "print(f'Random agent CTR  = {median_random_agent:.4f} ({lower_bound_random_agent:.4f}, {upper_bound_random_agent:.4f})')\n",
        "print(f'Best of agent CTR = {median_bestof_agent:.4f} ({lower_bound_bestof_agent:.4f}, {upper_bound_bestof_agent:.4f})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZX7ByUUDaDj"
      },
      "source": [
        "We see an improvement in the click-through rate for an agent as simple as the best of agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a-xT5spDaDj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}