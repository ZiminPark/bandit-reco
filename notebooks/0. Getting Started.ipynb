{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "0. Getting Started.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiminPark/bandit-reco/blob/master/notebooks/0.%20Getting%20Started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWmmhpKtDaDh"
      },
      "source": [
        "!pip install recogym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMt1yeoHDaDh"
      },
      "source": [
        "## In this notebook\n",
        "\n",
        "-  a simple recommendation agent that **suggests an item in proportion to how many times it has been viewed**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ0WSc_xDaDh"
      },
      "source": [
        "## Reinforcement Learning Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147cusrsDaDh"
      },
      "source": [
        "<img src=\"https://github.com/ZiminPark/bandit-reco/blob/master/notebooks/images/rl-setup.png?raw=1\" alt=\"Drawing\" style=\"width: 200px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKS9_LNA8CiE"
      },
      "source": [
        "## Organic and Bandit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_H1djTJDaDh"
      },
      "source": [
        "- Online advertising 위주로 다루지만, universal한 추천에 적용할 수 있는 프레임워크를 만들고 싶다.\n",
        "- 도메인에 무관한 용어 **_Organic_** and **_Bandit_** sessions을 사용하자.  \n",
        "    1. An **_Organic_** session is an observation of items the user interacts with.  For example, it could be views of products on an e-commerce website, listens to songs while streaming music, or readings of articles on an online newspaper.  \n",
        "    2. A **_Bandit_** session is one where we have an opportunity to recommend the user an item and observe their behavior.  We receive a reward if they click.\n",
        "\n",
        "<img src=\"https://github.com/ZiminPark/bandit-reco/blob/master/notebooks/images/organic-bandit.png?raw=1\" alt=\"Drawing\" style=\"width: 200px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F2YNvlpDaDh"
      },
      "source": [
        "## Offline and Online Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwNSTcKO8u8w"
      },
      "source": [
        "Criteo's 추천에 바로 RL을 적용할 수는 없었다. 학습하는 기간이 costly하기 때문. Instead, we need to leverage the vast amounts of offline training examples we already to make the algorithm perform as good as the current system before releasing it into the online production environment.\n",
        "\n",
        "Thus, RecoGym follows a similar flow. An agent is first given access to many offline training examples produced from a fixed policy. Then, they have access to the online system where they choose the actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x0_yDUyDaDh"
      },
      "source": [
        "<img src=\"https://github.com/ZiminPark/bandit-reco/blob/master/notebooks/images/two-steps.png?raw=1\" alt=\"Drawing\" style=\"width: 250px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnqqyW6PDaDi"
      },
      "source": [
        "## Let's see some code - Interacting with the environment \n",
        "\n",
        "\n",
        "The code snippet below shows how to initialize the environment and step through in an 'offline' manner (Here offline means that the environment is generating some recommendations for us).  We print out the results from the environment at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Blq14poDaDi"
      },
      "source": [
        "### World creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GqjZJTNDaDi"
      },
      "source": [
        "import gym, recogym\n",
        "from copy import deepcopy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IQnDfI7DaDi"
      },
      "source": [
        "# env_1_args is a dictionary of default parameters that defines the simulated world \n",
        "# such as user behavior, number of products, etc.\n",
        "from recogym import env_1_args, Configuration\n",
        "\n",
        "\n",
        "env_1_args['random_seed'] = 42\n",
        "env = gym.make('reco-gym-v1')\n",
        "env.init_gym(env_1_args)\n",
        "\n",
        "env.reset()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l7avCuTOuQd",
        "outputId": "420a0d62-d8a7-40df-e1e9-0fcc6dd9cc20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env_1_args"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'K': 5,\n",
              " 'change_omega_for_bandits': False,\n",
              " 'normalize_beta': False,\n",
              " 'num_clusters': 2,\n",
              " 'num_products': 10,\n",
              " 'num_users': 100,\n",
              " 'number_of_flips': 0,\n",
              " 'phi_var': 0.1,\n",
              " 'prob_bandit_to_organic': 0.05,\n",
              " 'prob_leave_bandit': 0.01,\n",
              " 'prob_leave_organic': 0.01,\n",
              " 'prob_organic_to_bandit': 0.25,\n",
              " 'random_seed': 42,\n",
              " 'sigma_mu_organic': 3,\n",
              " 'sigma_omega': 0.1,\n",
              " 'sigma_omega_initial': 1,\n",
              " 'with_ps_all': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRA_vktkDaDi"
      },
      "source": [
        "### Act on the environment\n",
        "We will now choose the product to recommend, and _hope_ for a click from the user.\n",
        "For our first agent we will hardcode the actions taken."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL420I62DaDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5f65e7-00d0-4f83-95ba-65b37787c803"
      },
      "source": [
        "actions = [None] + [1, 2, 3, 4, 5]  # Create a list of hard coded actions.\n",
        "env.reset()\n",
        "done = False  # Set Done to False.\n",
        "\n",
        "i = 0  # Counting how many steps.\n",
        "\n",
        "while not done and i < len(actions):\n",
        "    action = actions[i]\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    print(f\"Step: {i} - Action: {action} - Observation: {observation.sessions()} - Reward: {reward}\")\n",
        "    i += 1\n",
        "# 왜 어떤건 Observation이 있고 어떤건 없는지 모르겠네..."
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 - Action: None - Observation: [{'t': 0, 'u': 0, 'z': 'pageview', 'v': 3}, {'t': 1, 'u': 0, 'z': 'pageview', 'v': 3}, {'t': 2, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 3, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 4, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 5, 'u': 0, 'z': 'pageview', 'v': 3}, {'t': 6, 'u': 0, 'z': 'pageview', 'v': 5}] - Reward: None\n",
            "Step: 1 - Action: 1 - Observation: [{'t': 8, 'u': 0, 'z': 'pageview', 'v': 4}, {'t': 9, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 10, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 11, 'u': 0, 'z': 'pageview', 'v': 9}] - Reward: 0\n",
            "Step: 2 - Action: 2 - Observation: [] - Reward: 0\n",
            "Step: 3 - Action: 3 - Observation: [{'t': 14, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 15, 'u': 0, 'z': 'pageview', 'v': 9}, {'t': 16, 'u': 0, 'z': 'pageview', 'v': 9}] - Reward: 0\n",
            "Step: 4 - Action: 4 - Observation: [] - Reward: 0\n",
            "Step: 5 - Action: 5 - Observation: [] - Reward: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaNvIaDkOkuh"
      },
      "source": [
        "env.step?"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5P_MkvoDaDi"
      },
      "source": [
        "Okay, there's quite a bit going on here:  \n",
        "- `Action`\n",
        "   * `t` is the timestep (always incremented), it won't be useful today\n",
        "   * `u` is the user id, as we have one user, for now, it's always 0\n",
        "   * `a` is a number between `0` and `num_products - 1` that references the index of the product recommended.\n",
        "   * `ps` is the propensity score or the probability that the agent assigned to this action\n",
        "   * `ps-a` are the probabilities assigned to all actions by the agent (we can see that it's uniform for now: the agent randomly selects the recommended product)\n",
        "- `observation` will either be `None` or a session of Organic data, showing the index of products the user views.\n",
        "   * `t`, `u` have the same meaning as above\n",
        "   * `z` in the type of event (always pageview for now)\n",
        "   * `v` is the index of the viewed product\n",
        "- `reward` is `0` if the user does not click on the recommended product and `1` if they do.  Notice that when a user clicks on a product (Wherever the reward is `1`), they start a new Organic session.\n",
        "- `done` is a True/False flag indicating if the episode (aka user's timeline) is over.  \n",
        "\n",
        "Also, notice that the first `action` is `None`.  In our implementation, the agent observes Organic behavior before recommending anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCGTkfPEDaDi"
      },
      "source": [
        "## Creating our first agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BAEN2DfEJnt"
      },
      "source": [
        "- 객체 지향적으로 Agent를 만들어 보자.\n",
        "- 아래 코드는 Organic하게 가장 많이 본 아이템을 기록해두었다가 인기도에 비례해서 샘플하여 추천한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf8yKf84EKDW"
      },
      "source": [
        "import numpy as np\n",
        "from recogym.agents import Agent"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbIXnK2MFOQf"
      },
      "source": [
        "class BestOfAgent(Agent):\n",
        "    def __init__(self, config):\n",
        "        # Set number of products as an attribute of the Agent.\n",
        "        Agent.__init__(self, config)\n",
        "        self.organic_views = np.zeros(self.config.num_products)\n",
        "\n",
        "    def train(self, observation, action, reward, done):\n",
        "        \"\"\"Train method learns from a tuple of data.\n",
        "        this method can be called for offline or online learning\n",
        "        \"\"\"\n",
        "        # Adding organic session to organic view counts.\n",
        "        if observation:\n",
        "            for session in observation.sessions():\n",
        "                viewed_item_index = session['v']\n",
        "                self.organic_views[viewed_item_index] += 1\n",
        "\n",
        "    def act(self, observation, reward, done):\n",
        "        \"\"\"returns an action based on current observation and past history\"\"\"\n",
        "        \n",
        "        probabilities = self.organic_views / np.sum(self.organic_views)\n",
        "        action = np.random.choice(self.config.num_products, p=probabilities)\n",
        "        \n",
        "        return {\n",
        "            **super().act(observation, reward, done),\n",
        "            **{\n",
        "                'a': action,\n",
        "                'ps': probabilities[action],\n",
        "                'ps-a': probabilities,\n",
        "            }\n",
        "        }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a5AlyuTFW3l"
      },
      "source": [
        "- BestOfAgent class가 우리가 선호하는 Agent 생성 방식이다.\n",
        "- `train` 메소드는 take in training data from the environments step_offline method and thus has nothing to return\n",
        "- `act` 메소드는 return an action to pass back into the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exa_kiTJDaDj"
      },
      "source": [
        "아래 코드에서 이 Agent가 어떻게 처음 first offline training하고 이걸 이용해서 online으로 추천하는지 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdgFBtGeHyMu"
      },
      "source": [
        "### 내가 찍어본 코드."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkD-_M6bJRNz"
      },
      "source": [
        "- step_offline이 뭐하는 애인가 싶어서. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GokClSJeGb8X",
        "outputId": "d52f2fb8-c1d3-4d18-e1ce-16ad7b6bed5a"
      },
      "source": [
        "env.reset()\n",
        "observation, reward, done = None, 0, False\n",
        "action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
        "observation.sessions()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'t': 0, 'u': 0, 'v': 5, 'z': 'pageview'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "CRygiB84I1g7",
        "outputId": "c0c29076-14e1-4210-8ae9-53af978781ed"
      },
      "source": [
        "env.reset()\n",
        "observation, reward, done = None, 0, False\n",
        "action, observation, reward, done, info = env.step(observation, reward, done)  # 그냥 step은 당연하게도 action만 받는다.\n",
        "observation.sessions()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a13d940981c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 그냥 step은 당연하게도 action만 받는다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: step() takes 2 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0sjoueNH3XB"
      },
      "source": [
        "### 원래 자료 다시 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLgbWCLHDaDj"
      },
      "source": [
        "# Instantiate instance of PopularityAgent class.\n",
        "num_products = 10\n",
        "agent = BestOfAgent(Configuration({**env_1_args, 'num_products': num_products}))\n",
        "env.reset_random_seed()\n",
        "\n",
        "# Train on 1000 users offline.\n",
        "num_offline_users = 1000\n",
        "\n",
        "for _ in range(num_offline_users):\n",
        "\n",
        "    env.reset()\n",
        "    done = False\n",
        "\n",
        "    observation, reward, done = None, 0, False\n",
        "    while not done:\n",
        "        old_observation = observation\n",
        "        action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
        "        agent.train(old_observation, action, reward, done)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyM2ClLP6BB",
        "outputId": "bf6fa5a9-22f3-4e89-e17f-1cd7431589a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "agent.act(old_observation, reward, done)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 9,\n",
              " 'ps': 0.2729454956227054,\n",
              " 'ps-a': array([0.09145251, 0.02306316, 0.00381248, 0.06918949, 0.29233738,\n",
              "        0.14143839, 0.01572061, 0.05963475, 0.03040572, 0.2729455 ]),\n",
              " 't': 84,\n",
              " 'u': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUQtwUEkHc5q",
        "outputId": "d670a55e-8d7e-46b4-bba1-8076b42c7d59"
      },
      "source": [
        "agent.organic_views"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1943.,  490.,   81., 1470., 6211., 3005.,  334., 1267.,  646.,\n",
              "       5799.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRo5SZ2sG-05"
      },
      "source": [
        "# Train on 100 users online and track click through rate.\n",
        "num_online_users = 100\n",
        "num_clicks, num_events = 0, 0\n",
        "\n",
        "for _ in range(num_online_users):\n",
        "\n",
        "    # Reset env and set done to False.\n",
        "    env.reset()\n",
        "    observation, _, done, _ = env.step(None)\n",
        "    reward = None\n",
        "    done = None\n",
        "    while not done:\n",
        "        action = agent.act(observation, reward, done)\n",
        "        observation, reward, done, info = env.step(action['a'])\n",
        "\n",
        "        # Used for calculating click through rate.\n",
        "        num_clicks += 1 if reward == 1 else 0\n",
        "        num_events += 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLZMhK11J7bn",
        "outputId": "2072708e-d809-4988-e529-d95a84dcf23d"
      },
      "source": [
        "num_events"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8163"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRc92Nt9HBBk",
        "outputId": "e4d4accd-b00b-45e0-aebc-8dc3ee8e7501"
      },
      "source": [
        "ctr = num_clicks / num_events\n",
        "print(f\"Click Through Rate: {ctr:.4f}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Click Through Rate: 0.0141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTXKnE_TDaDj"
      },
      "source": [
        "## Testing our first agent\n",
        "\n",
        "- baseline으로 아이템을 똑같은 확률로 추천하는 에이전트(`RandomAgent`)를 사용해보자.\n",
        "- 이를 위해 조금 더 복잡한 toy 환경인 `reco-gym-v1`을 사용하겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJJZC8UHDaDj"
      },
      "source": [
        "import gym, recogym\n",
        "from recogym import env_1_args\n",
        "from recogym.agents import RandomAgent, random_args\n",
        "from copy import deepcopy\n",
        "\n",
        "env_1_args['random_seed'] = 42\n",
        "env_1 = gym.make('reco-gym-v1')\n",
        "env_1.init_gym(env_1_args)\n",
        "\n",
        "# Create the two agents.\n",
        "num_products = env_1_args['num_products']\n",
        "\n",
        "best_of_agent = BestOfAgent(Configuration(env_1_args))\n",
        "random_agent = RandomAgent(Configuration({**env_1_args, **random_args,}))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyMKCQkUDaDj"
      },
      "source": [
        "- 비교를 위해 `test_agent` 메소드를 사용할 수 있다.\n",
        "- `test_agent`를 사용하려면 아래의 내용을 제공해야 한다.\n",
        "    1. a copy of the current env \n",
        "    2. a copy of the agent class\n",
        "    3. the number of training users\n",
        "    4. the number of testing users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZLXFqamL--e"
      },
      "source": [
        "recogym.test_agent?"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbITkmXXDaDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ca3fe0-d33a-4c7f-8184-60225d68dadd"
      },
      "source": [
        "# Confidence interval of the CTR median and 0.025 0.975 quantile.\n",
        "random_agent_results = recogym.test_agent(\n",
        "    deepcopy(env_1),\n",
        "    deepcopy(random_agent),\n",
        "    num_offline_users=1000,\n",
        "    num_online_users=1000\n",
        ")\n",
        "median_random_agent, lower_bound_random_agent, upper_bound_random_agent = random_agent_results"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Organic Users: 100%|██████████| 100/100 [00:00<00:00, 2255.27it/s]\n",
            "Users:   2%|▏         | 16/1000 [00:00<00:14, 69.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "START: Agent Training #0\n",
            "START: Agent Training @ Epoch #0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Users: 100%|██████████| 1000/1000 [00:22<00:00, 44.78it/s]\n",
            "Organic Users: 0it [00:00, ?it/s]\n",
            "Users:   2%|▏         | 16/1000 [00:00<00:19, 49.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "END: Agent Training @ Epoch #0 (22.391456365585327s)\n",
            "START: Agent Evaluating @ Epoch #0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Users: 100%|██████████| 1000/1000 [00:22<00:00, 44.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "END: Agent Evaluating @ Epoch #0 (22.579938411712646s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6OuEdjXDaDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ac41d8-e414-4f4f-801e-a30551e74856"
      },
      "source": [
        "# Confidence interval of the CTR median and 0.025 0.975 quantile.\n",
        "bestof_agent_results = recogym.test_agent(\n",
        "    deepcopy(env),\n",
        "    deepcopy(best_of_agent),\n",
        "    num_offline_users=1000,\n",
        "    num_online_users=1000\n",
        ")\n",
        "median_bestof_agent, lower_bound_bestof_agent, upper_bound_bestof_agent = bestof_agent_results"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Organic Users: 100%|██████████| 100/100 [00:00<00:00, 1245.40it/s]\n",
            "Users:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "START: Agent Training #0\n",
            "START: Agent Training @ Epoch #0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Users: 100%|██████████| 1000/1000 [00:22<00:00, 44.01it/s]\n",
            "Organic Users: 0it [00:00, ?it/s]\n",
            "Users:   1%|          | 12/1000 [00:00<00:19, 49.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "END: Agent Training @ Epoch #0 (22.80797266960144s)\n",
            "START: Agent Evaluating @ Epoch #0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Users: 100%|██████████| 1000/1000 [00:28<00:00, 35.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "END: Agent Evaluating @ Epoch #0 (28.645923614501953s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlnDnVTSDaDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f4922d-1116-4a11-c578-a1acf437d740"
      },
      "source": [
        "print(f'Random agent CTR  = {median_random_agent:.4f} ({lower_bound_random_agent:.4f}, {upper_bound_random_agent:.4f})')\n",
        "print(f'Best of agent CTR = {median_bestof_agent:.4f} ({lower_bound_bestof_agent:.4f}, {upper_bound_bestof_agent:.4f})')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random agent CTR  = 0.0109 (0.0102, 0.0117)\n",
            "Best of agent CTR = 0.0147 (0.0138, 0.0155)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZX7ByUUDaDj"
      },
      "source": [
        "We see an improvement in the click-through rate for an agent as simple as the best of agent."
      ]
    }
  ]
}