{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install recogym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer's curse\n",
    "\n",
    "We will use the same likelihood model built in the previous notebook to showcase optimizer's curse and how that can even lead to an inversion of arms (taking wrong decision).\n",
    "\n",
    "Let's first setup, as usual, the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recogym.envs.session import OrganicSessions\n",
    "\n",
    "from numpy.random.mtrand import RandomState\n",
    "from recogym import Configuration\n",
    "from recogym.agents import Agent\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from recogym import verify_agents\n",
    "from recogym.agents import OrganicUserEventCounterAgent, organic_user_count_args\n",
    "from recogym.evaluate_agent import verify_agents, plot_verify_agents\n",
    "\n",
    "import gym, recogym\n",
    "from copy import deepcopy\n",
    "from recogym import env_1_args\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['figure.figsize'] = [6, 3]\n",
    "\n",
    "num_users = 5000\n",
    "num_products = 10\n",
    "\n",
    "env_1_args['random_seed'] = 42\n",
    "env_1_args['num_products'] = num_products\n",
    "\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a logging policy that shows products with a probability proportional to their popularity (very plausible world model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USERS = 1000\n",
    "NUM_PRODUCTS = 10\n",
    "\n",
    "organic_counter_agent = OrganicUserEventCounterAgent(Configuration({\n",
    "           **organic_user_count_args,\n",
    "           **env_1_args,\n",
    "           'select_randomly': True,\n",
    "       }))\n",
    "\n",
    "popularity_policy_logs = env.generate_logs(NUM_USERS, organic_counter_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use our product views feature provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recogym.agents import FeatureProvider\n",
    "\n",
    "class CountFeatureProvider(FeatureProvider):\n",
    "    \"\"\"Feature provider as an abstract class that defined interface of setting/getting features\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(CountFeatureProvider, self).__init__(config)\n",
    "        self.feature_data = np.zeros((self.config.num_products))\n",
    "\n",
    "    def observe(self, observation):\n",
    "        \"\"\"Consider an Organic Event for a particular user\"\"\"\n",
    "        for session in observation.sessions():\n",
    "            self.feature_data[int(session['v'])] += 1\n",
    "\n",
    "    def features(self, observation):\n",
    "        \"\"\"Provide feature values adjusted to a particular feature set\"\"\"\n",
    "        return self.feature_data\n",
    "\n",
    "    def reset(self):\n",
    "        self.feature_data = np.zeros((self.config.num_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from recogym import Configuration, DefaultContext, Observation\n",
    "\n",
    "def build_train_data(logs, feature_provider):\n",
    "    user_states, actions, rewards, proba_actions = [], [], [], []\n",
    "    \n",
    "    current_user = None\n",
    "    for _, row in logs.iterrows():\n",
    "        if current_user != row['u']:\n",
    "            # Use has changed: start a new session and reset user state\n",
    "            current_user = row['u']\n",
    "            sessions = OrganicSessions()\n",
    "            feature_provider.reset()\n",
    "        \n",
    "        context = DefaultContext(row['u'], row['t'])\n",
    "        \n",
    "        if row['z'] == 'organic':\n",
    "            sessions.next(context, row['v'])\n",
    "            \n",
    "        else:\n",
    "            # For each bandit event, generate one observation for the user state, the taken action\n",
    "            # the obtained reward and the used probabilities\n",
    "            feature_provider.observe(Observation(context, sessions))\n",
    "            user_states.append(feature_provider.features(None))\n",
    "            actions.append(row['a'])\n",
    "            rewards.append(row['c'])\n",
    "            proba_actions.append(row['ps'])\n",
    "            \n",
    "            # Start a new organic session\n",
    "            sessions = OrganicSessions()\n",
    "    \n",
    "    return np.array(user_states), np.array(actions).astype(int), np.array(rewards), np.array(proba_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now see data that will be provided to our agents based on logistic regressions\n",
    "config = Configuration(env_1_args)\n",
    "count_feature_provider = CountFeatureProvider(config=config)\n",
    "\n",
    "user_states, actions, rewards, proba_actions = build_train_data(popularity_policy_logs, count_feature_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_start, preview_size = 500, 3\n",
    "\n",
    "print('user product views count at action time')\n",
    "print(user_states[preview_start:preview_start + preview_size])\n",
    "print('taken actions', actions[preview_start:preview_start + preview_size])\n",
    "print('obtained rewards', rewards[preview_start:preview_start + preview_size])\n",
    "print('probablities of the taken actions', proba_actions[preview_start:preview_start + preview_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LikelihoodAgent(Agent):\n",
    "    def __init__(self, feature_provider, use_argmax=False, seed=43):\n",
    "        self.feature_provider = feature_provider\n",
    "        self.use_argmax = use_argmax\n",
    "        self.random_state = RandomState(seed)\n",
    "        self.model = None\n",
    "        \n",
    "    @property\n",
    "    def num_products(self):\n",
    "        return self.feature_provider.config.num_products\n",
    "    \n",
    "    def _create_features(self, user_state, action):\n",
    "        \"\"\"Create the features that are used to estimate the expected reward from the user state.\n",
    "        \"\"\"\n",
    "        features = np.zeros(len(user_state) * self.num_products)\n",
    "        features[action * len(user_state): (action + 1) * len(user_state)] = user_state\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, logs):\n",
    "        user_states, actions, rewards, proba_actions = build_train_data(logs, self.feature_provider)\n",
    "        \n",
    "        features = np.vstack([\n",
    "            self._create_features(user_state, action) \n",
    "            for user_state, action in zip(user_states, actions)\n",
    "        ])\n",
    "        self.model = LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "        self.model.fit(features, rewards)\n",
    "    \n",
    "    def _score_products(self, user_state):\n",
    "        all_action_features = np.array([\n",
    "            self._create_features(user_state, action) \n",
    "            for action in range(self.num_products)\n",
    "        ])\n",
    "        return self.model.predict_proba(all_action_features)[:, 1]\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"Act method returns an action based on current observation and past history\"\"\"\n",
    "        self.feature_provider.observe(observation)        \n",
    "        user_state = self.feature_provider.features(observation)\n",
    "        prob = self._score_products(user_state)\n",
    "        \n",
    "        if not self.use_argmax:\n",
    "            action = self.random_state.choice(self.num_products, p=(prob / np.sum(prob)))\n",
    "            ps = prob[action]\n",
    "            all_ps = prob.copy()\n",
    "        else:\n",
    "            action = np.argmax(prob)\n",
    "            ps = 1.0\n",
    "            all_ps = np.zeros(self.num_products)\n",
    "            all_ps[action] = 1.0\n",
    "      \n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'expected-value': prob[action],\n",
    "                'ps': ps,\n",
    "                'ps-a': all_ps,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.feature_provider.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, num_users, agent):\n",
    "    \"\"\"Small utility function to collect stats about you agent on simulated test traffic\n",
    "    It is really recogym specific, you do not need to look at its internal details\n",
    "    \"\"\"\n",
    "    env = deepcopy(env)\n",
    "    env.agent = agent  \n",
    "    \n",
    "    events = []\n",
    "    for user_id in range(num_users):\n",
    "        env.reset(user_id)\n",
    "        observation, reward, done, _ = env.step(None)\n",
    "\n",
    "        while not done:\n",
    "            for session in observation.sessions():\n",
    "                events += [{**session, 'z': 'organic'}]\n",
    "\n",
    "            action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
    "            events += [{**action, 'z': 'bandit', 'c': reward}]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    events_df = pd.DataFrame().from_dict(events)\n",
    "    ordered_cols = ['t', 'u', 'z', 'v', 'a', 'c', 'ps', 'ps-a']\n",
    "    all_cols = ordered_cols + [col for col in events_df.columns if col not in ordered_cols]\n",
    "    return events_df[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the feature vector used by the Likelihood agent\n",
    "picked_sample = 500\n",
    "\n",
    "count_product_views_feature_provider = CountFeatureProvider(config)\n",
    "likelihood_logreg = LikelihoodAgent(count_product_views_feature_provider)\n",
    "\n",
    "print('User state', user_states[picked_sample])\n",
    "print('action', actions[picked_sample])\n",
    "print('Created cross features')\n",
    "print(likelihood_logreg._create_features(user_states[picked_sample], actions[picked_sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "likelihood_logreg = LikelihoodAgent(count_product_views_feature_provider, use_argmax=True)\n",
    "likelihood_logreg.train(popularity_policy_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_users = 1000\n",
    "likelihood_logreg_test_logs = run_agent(env, n_test_users, likelihood_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the predicted number of clicks VS the actual number\n",
    "\n",
    "We can see that the model over-predicted the number of clicks, and even inversed the order of products 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# To force the arms to share the same colors\n",
    "palette = {c: f'C{c}' for c in range(NUM_PRODUCTS)} if NUM_PRODUCTS < 20 else None\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4), sharey=True)\n",
    "sns.barplot(x=\"a\", y=\"expected-value\", data=likelihood_logreg_test_logs, ax=axes[0], estimator=sum, palette=palette)\n",
    "axes[0].set_title('Expected number of clicks')\n",
    "axes[0].set_xlabel('Selected product')\n",
    "\n",
    "sns.barplot(x=\"a\", y=\"c\", data=likelihood_logreg_test_logs, ax=axes[1], estimator=sum, palette=palette)\n",
    "axes[1].set_title('Obtained number of clicks')\n",
    "axes[1].set_xlabel('Selected product')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
