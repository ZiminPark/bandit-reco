{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Risk Minimisation (CRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install recogym torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/bamine/recsys-summer-school.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('recsys-summer-school/notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from recogym import env_1_args, Configuration\n",
    "from recogym import Configuration\n",
    "from recogym.agents import Agent\n",
    "from recogym.envs.observation import Observation\n",
    "from recogym.agents import RandomAgent, random_args\n",
    "from recogym import verify_agents, verify_agents_IPS\n",
    "from recogym.evaluate_agent import plot_verify_agents, verify_agents_recall_at_k\n",
    "\n",
    "from util import FullBatchLBFGS\n",
    "from util import OrganicUserEventCounterAgent, organic_user_count_args\n",
    "\n",
    "# Set style for pretty plots\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "P = 10  # Number of Products\n",
    "U = 5000 # Number of Users\n",
    "\n",
    "# You can overwrite environment arguments here:\n",
    "env_1_args['random_seed'] = 42\n",
    "env_1_args['num_products']= P\n",
    "env_1_args['phi_var']=0.0\n",
    "env_1_args['number_of_flips']=P//2\n",
    "env_1_args['sigma_mu_organic'] =3 #0.1\n",
    "env_1_args['sigma_omega']=0.1 #0.05\n",
    "\n",
    "# Initialize the gym for the first time by calling .make() and .init_gym()\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate RecSys logs for U users\n",
    "logger = OrganicUserEventCounterAgent(Configuration({**organic_user_count_args,\n",
    "                                                     'select_randomly': True,\n",
    "                                                     'epsilon': .01}))\n",
    "reco_log = env.generate_logs(U, agent = logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_log.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandits\n",
    "Classical value-based methods aim to learn the probability that a given action will lead to a positive reward, that is:\n",
    "\n",
    "$$p(c = 1|\\mathbf{x},a)$$\n",
    "\n",
    "In what follows, we will implement a different approach: a policy learning method.\n",
    "Policy-learning methods do not explicitly model the probability of a positive reward, but aim to directly model the action that should be taken, given a context:\n",
    "\n",
    "$$p(a|\\mathbf{x})$$\n",
    "\n",
    "Classical contextual bandits achieve this by optimising the expectation of the reward under the new (learned) policy $\\pi_\\theta$, given a logged dataset under policy $\\pi_0$.\n",
    "For a given dataset $\\mathcal{D}$ consisting of $N$ tuples $(\\mathbf{x},a,p,c)$, the objective can be written as the following:\n",
    "\n",
    "$$\\theta^{*} = \\text{argmax}_{\\theta} \\sum_{i=1}^{N}c_i\\frac{\\pi_\\theta(a_i|\\mathbf{x}_i)}{\\pi_0(a_i|\\mathbf{x}_i)}$$\n",
    "\n",
    "This objective can be straightforwardly optimised using your favourite package that provides auto-differentiation functionality.\n",
    "In our example, we will use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultinomialLogisticRegressionModel, self).__init__()\n",
    "        # Generate weights - initialise randomly\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a = np.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute linear transformation x.A.T\n",
    "        pred = F.linear(x, self.weight)\n",
    "        return pred\n",
    "\n",
    "class ContextualBandit(Agent):\n",
    "    def __init__(self, config, U = U, P = P, log = False, svp_lambda = 0.0):\n",
    "        super(ContextualBandit, self).__init__(config)\n",
    "        self.model = MultinomialLogisticRegressionModel(P, P)\n",
    "        self.loss_history = []\n",
    "        self.user_state = np.zeros(P)\n",
    "        self.U = U\n",
    "        self.P = P\n",
    "        self.log = log\n",
    "        self.svp_lambda = svp_lambda\n",
    "\n",
    "    def train(self, logs):\n",
    "        ''' Train the contextual bandit based on an offline log '''\n",
    "        # Preprocess the log, we want tuples of the form (x,a,p,c) where:\n",
    "        #     - x is a context vector (counts of organic views)\n",
    "        #     - a is the action that was taken\n",
    "        #     - p is the logging propensity for that action\n",
    "        #     - c is the observed reward\n",
    "        X = []\n",
    "        a = []\n",
    "        p = []\n",
    "        c = []\n",
    "        # For every log\n",
    "        user_id = 0\n",
    "        current_context = np.zeros(P)\n",
    "        for row in logs.itertuples():\n",
    "            # Is this a new user?\n",
    "            if row.u != user_id:\n",
    "                # Reset the user state\n",
    "                current_context = np.zeros(P)\n",
    "                user_id = row.u\n",
    "            \n",
    "            # Is this an organic view?\n",
    "            if row.z == 'organic':\n",
    "                # Update the user state\n",
    "                current_context[int(row.v)] += 1\n",
    "            # Is this a bandit view?\n",
    "            elif row.z == 'bandit':\n",
    "                # Add tuple to training data\n",
    "                X.append(current_context)\n",
    "                a.append(int(row.a))\n",
    "                p.append(row.ps)\n",
    "                c.append(row.c)\n",
    "        # Turn into Numpy arrays for easy indexing\n",
    "        X = np.asarray(X)\n",
    "        a = np.asarray(a)\n",
    "        p = np.asarray(p)\n",
    "        c = np.asarray(c)\n",
    "        N = np.sum(c)\n",
    "        \n",
    "        # Put into PyTorch variables - drop unclicked samples\n",
    "        X = Variable(torch.Tensor(X[c != 0]))\n",
    "        a = Variable(torch.LongTensor(a[c != 0]))\n",
    "        w = torch.Tensor(p[c != 0] ** -1)\n",
    "        \n",
    "        def closure():\n",
    "            # Reset gradients\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Compute action predictions for clicks\n",
    "            p_a = self.model(X)\n",
    "            \n",
    "            # Turn these into probabilities through softmax\n",
    "            p_a = F.softmax(p_a, dim = 1)\n",
    "            \n",
    "            # Only keep probabilities for the actions that were taken\n",
    "            p_a = torch.gather(p_a, 1, a.unsqueeze(1))\n",
    "            \n",
    "            # IPS reweighting and logging\n",
    "            reward = p_a * w\n",
    "            log_reward = torch.log(torch.clamp(p_a, min = 1e-12)) * w    \n",
    "        \n",
    "            # PyTorch likes to minimise - loss instead of reward\n",
    "            if self.log:\n",
    "                loss = -log_reward\n",
    "            else:\n",
    "                loss = -reward\n",
    "            \n",
    "            # Sample Variance Penalisation\n",
    "            var = .0\n",
    "            if self.svp_lambda:\n",
    "                # Compute the expectation of the IPS estimate\n",
    "                avg_weighted_loss = torch.mean(-reward)\n",
    "\n",
    "                # Compute the variance of the IPS estimate\n",
    "                var = torch.sqrt(torch.sum(((-reward) - avg_weighted_loss)**2) / (N - 1) / N)\n",
    "            \n",
    "            # Reweight with lambda and add to the loss\n",
    "            loss = loss.mean() + self.svp_lambda * var\n",
    "\n",
    "            return loss\n",
    "        \n",
    "        # Set up optimiser\n",
    "        optimiser = FullBatchLBFGS(self.model.parameters())\n",
    "\n",
    "        # Initial loss\n",
    "        self.loss_history.append(closure())\n",
    "        max_epoch = 50\n",
    "        for epoch in tqdm(range(max_epoch)):\n",
    "            # Optimisation step\n",
    "            obj, _, _, _, _, _, _, _ = optimiser.step({'closure': closure,\n",
    "                                                       'current_loss': self.loss_history[-1],\n",
    "                                                       'max_ls': 20})\n",
    "            self.loss_history.append(obj)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        ''' Plot the training loss over epochs '''\n",
    "        _,_ = plt.subplots()\n",
    "        plt.plot(range(len(self.loss_history)),self.loss_history)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        ''' Observe new organic views and capture them in the user state '''\n",
    "        for session in observation.sessions():\n",
    "            self.user_state[int(session['v'])] += 1\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        ''' Pick an action, based on the current observation and the history '''\n",
    "        # Observe\n",
    "        self.observe(observation)\n",
    "\n",
    "        # Act\n",
    "        p_a = self.model(torch.Tensor(self.user_state)).detach().numpy().ravel()\n",
    "        action = np.argmax(p_a)\n",
    "        prob = np.zeros_like(p_a)\n",
    "        prob[action]=1.0\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'ps': 1.0,\n",
    "                'ps-a': prob,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        ''' Reset the user state '''\n",
    "        self.user_state = np.zeros(self.P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Contextual Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cb = ContextualBandit({}, log = False)\n",
    "cb.train(reco_log)\n",
    "cb.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concave Contextual Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_cb = ContextualBandit({}, log = True)\n",
    "log_cb.train(reco_log)\n",
    "log_cb.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organic_counter_agent = OrganicUserEventCounterAgent(Configuration({**organic_user_count_args,\n",
    "                                                     'select_randomly': False,\n",
    "                                                     'epsilon': 0.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_AB = verify_agents(env,\n",
    "                          5000,\n",
    "                          {' User-pop': organic_counter_agent,\n",
    "                           'Contextual Bandit': cb,\n",
    "                           'Concave Contextual Bandit': log_cb,\n",
    "                          })\n",
    "result_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_verify_agents(result_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POEM\n",
    "\n",
    "The classical IPS estimator has its disadvantages, however.\n",
    "In its current form, the variance of the estimate can grow to be of significant size.\n",
    "To mitigate this, Swaminathan and Joachims propose to include a sample variance penalisation term to the objective, effectively ensuring that the learned model does not stray too far from the logging policy.\n",
    "\n",
    "This is the Counterfactual Risk Minimisation (CRM) objective, and the learning method optimising it directly is called POEM:\n",
    "$$\\theta^{*} = \\text{argmax}_{\\theta} \\sum_{i=1}^{N}c_i\\frac{\\pi_\\theta(a_i|\\mathbf{x}_i)}{\\pi_0(a_i|\\mathbf{x}_i)} - \\lambda \\sqrt{\\frac{\\widehat{Var}_\\theta}{N}} $$\n",
    "\n",
    "\n",
    "We model $\\pi_\\theta$ as linear:\n",
    "\n",
    "$$\\pi_\\theta(a|\\mathbf{x}) = \\text{softmax}(\\mathbf{x}^{\\intercal}\\theta)_a$$\n",
    "\n",
    "\n",
    "Swaminathan, Adith, and Thorsten Joachims. \"Batch learning from logged bandit feedback through counterfactual risk minimization.\" Journal of Machine Learning Research 16.1 (2015): 1731-1755."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical POEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poem = ContextualBandit({}, log = False, svp_lambda = .5)\n",
    "poem.train(reco_log)\n",
    "poem.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concave POEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_poem = ContextualBandit({}, log = True, svp_lambda = 10)\n",
    "log_poem.train(reco_log)\n",
    "log_poem.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_AB = verify_agents(env,\n",
    "                          5000,\n",
    "                          {' User-pop': organic_counter_agent,\n",
    "                           'Contextual Bandit': cb,\n",
    "                           'Log-Contextual Bandit': log_cb,\n",
    "                           'POEM': poem,\n",
    "                           'Log-POEM': log_poem,\n",
    "                          })\n",
    "result_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_verify_agents(result_AB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
