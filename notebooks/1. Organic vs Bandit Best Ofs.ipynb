{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If recogym is not yet installed in your environment, run:\n",
    "!pip install recogym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit Feedback - Organic Best of vs. Bandit Best of\n",
    "\n",
    "In this notebook, we use the bandit signal (how users react to ads) for the first time, in the simplest possible way.  We compare how a recommender system that always makes the most popular organic product as a recommended with a recommender system that always makes the most popular bandit product.\n",
    "\n",
    "We see that there can be differences in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, recogym\n",
    "from recogym import env_1_args, Configuration\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6, 3]\n",
    "\n",
    "num_users = 1000\n",
    "num_products = 10\n",
    "\n",
    "env_1_args['sigma_mu_organic'] = 2\n",
    "env_1_args['sigma_omega'] = 2.\n",
    "env_1_args['random_seed'] = 42\n",
    "env_1_args['num_products'] = num_products\n",
    "env_1_args['K'] = 2\n",
    "env_1_args['number_of_flips'] = 2\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can generate data for many user at once with env.generate_logs\n",
    "data = env.generate_logs(num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data the following columns are present:\n",
    "* **`t`**—_`Time`_. Currently, _`Time`_ reflects the order of _`Events`_, but it does **not** represent the notion of the time in a physical sense.\n",
    "* **`u`**—_`User`_.\n",
    "* **`z`**—_`Event Type`_. There are two types of _`Events`_: _Organic_ and _Bandit_.\n",
    "* **`v`**—_`View`_. The column shows which _`Product`_ was shown to a _`User`_ in an _Organic_ _`Event`_.\n",
    "* **`a`**—_`Action`_. Currently, _`Action`_ is a _`Product`_ that was provided to a _`User`_ during a _Bandit_ _`Event`_.\n",
    "* **`c`**—_`Click`_. It is a _Reward_ for an _`Action`_ provided by the _`Agent`_.\n",
    "* **`ps`**—Probability of selecting a particular _`Action`_.\n",
    "\n",
    "**Note #1:** _`Time`_ , _`User`_ , _`Views`_ , and _`Actions`_ **all** they start with _`0`_.\n",
    "\n",
    "**Note #2:** For any _`User`_, _Organic_ _`Event`_ _**always**_ precedes a _Bandit_ _`Event`_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "Let's analyze these histograms:\n",
    "* _Actions per Product_\n",
    "* _Clicks per Product_\n",
    "* _Views per Product_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Number of actions per product')\n",
    "taken_actions = data[data['z'] == 'bandit']['a']\n",
    "actions, action_counts = np.unique(taken_actions, return_counts=True)\n",
    "plt.bar(actions, action_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All _`Actions`_ are _*evenly*_ distributed: each product is as likely to be recommended since the logs come from the realization of a random policy (the default random agent).\n",
    "It is not typical behavior (usually actions are personalized, not random), and we will relax this assumption later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewarded_actions = # code here\n",
    "actions, total_rewards = np.unique(rewarded_actions, return_counts=True)\n",
    "\n",
    "plt.title('Histogram of clicks per product')\n",
    "plt.bar(actions, total_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate a click-through rate for each recommendation by dividing the number of times we obtained a click by the number of impressions.  This produces the following \"bandit best of\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import beta\n",
    "\n",
    "def get_beta_confidence_interval(n_impressions, n_clicks):\n",
    "    n_unclicked = n_impressions - n_clicks\n",
    "    low_quantile = beta.ppf(0.025, n_clicks + 1, n_unclicked + 1)\n",
    "    median = beta.ppf(0.500, n_clicks + 1, n_unclicked + 1)\n",
    "    high_quantile = beta.ppf(0.975, n_clicks + 1, n_unclicked + 1)\n",
    "    return median - low_quantile, high_quantile - median\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = np.zeros(num_products)\n",
    "lower_errors = np.zeros(num_products)\n",
    "upper_errors = np.zeros(num_products)\n",
    "\n",
    "for product_id in actions.astype(int):\n",
    "    n_impressions = action_counts[product_id]\n",
    "    n_clicks = # code here\n",
    "    \n",
    "    lower_bound, upper_bound = get_beta_confidence_interval(n_impressions, n_clicks)\n",
    "    \n",
    "    ctr[product_id] = n_clicks / n_impressions\n",
    "    lower_errors[product_id] = lower_bound\n",
    "    upper_errors[product_id] = upper_bound\n",
    "\n",
    "plt.title('Click through rate (non personalised)')\n",
    "\n",
    "plt.errorbar(\n",
    "    actions, ctr, yerr=(lower_errors, upper_errors),\n",
    "    fmt='o', ecolor='darkred', capsize=4)\n",
    "\n",
    "plt.xlabel('Product ID')\n",
    "plt.ylabel('Click-through rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandit based\n",
    "The non-personalised click-through rate can be used as a non-personalised agent.  It will be our first likelihood-based agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from recogym.agents import Agent\n",
    "\n",
    "class SingleActionAgent(Agent):\n",
    "    def __init__(self, preferred_action, config = Configuration({'num_products': 10})):\n",
    "        Agent.__init__(self, config)\n",
    "        self.preferred_action = preferred_action\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        probabilities = np.zeros(self.config.num_products)\n",
    "        probabilities[self.preferred_action] = 1.\n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': self.preferred_action,\n",
    "                'ps': probabilities[self.preferred_action],\n",
    "                'ps-a': probabilities,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise your agent.\n",
    "top_ctr_item = # code here\n",
    "print('The Greedy-Bandit agent will always take action {0}'.format(top_ctr_item))\n",
    "greedy_bandit = SingleActionAgent(top_ctr_item, Configuration(env_1_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organic based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ads that attract the most clicks are not, in general, the same as the products that are often viewed organically, to see this, we can plot the organic views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_views =  data[data['z'] == 'organic']['v']\n",
    "products, views_counts = np.unique(product_views, return_counts=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 3))\n",
    "axes[0].set_title('Histogram of Views per Product')\n",
    "axes[0].bar(products, views_counts)\n",
    "\n",
    "axes[1].set_title('Histogram of CTR per Product')\n",
    "axes[1].bar(products, ctr)\n",
    "\n",
    "sns.regplot(views_counts, ctr, ax=axes[2])\n",
    "axes[2].set_title('CTR for number of organic views')\n",
    "axes[2].set_xlabel('Number of organic views')\n",
    "axes[2].set_ylabel('Click-through rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there may be a relationship between popular products and a high click-through rate, it is certainly not guaranteed.  This fact underlies the need to use bandit feedback.  Here we will develop our final organic agent as an organic best-of to hammer this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_viewed_item = # code here\n",
    "print('The organic best-of agent will always take action {0}'.format(top_viewed_item))\n",
    "greedy_organic = SingleActionAgent(top_viewed_item, Configuration(env_1_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from recogym.agents import RandomAgent, random_args\n",
    "from recogym.evaluate_agent import verify_agents, plot_verify_agents\n",
    "\n",
    "random_agent = RandomAgent(Configuration({\n",
    "    **env_1_args,\n",
    "    **random_args,\n",
    "}))\n",
    "\n",
    "result = verify_agents(\n",
    "    env, number_of_users = 1000, # try with 5000 for more significant results\n",
    "    agents = {\n",
    "        'Random Agent': random_agent,\n",
    "        'Greedy-Organic aka. best-of': greedy_organic,\n",
    "        'Greedy-Bandit': greedy_bandit,\n",
    "   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_verify_agents(result)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, the bandit agent is better than best-of and random agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
